import logging
import uuid
from typing import Dict
from django.conf import settings
import google.generativeai as genai
from google.cloud import firestore
from google.cloud.firestore_v1.base_query import FieldFilter
from google.oauth2 import service_account
import os
import json

logger = logging.getLogger(__name__)

cred_path = os.environ.get("FIRESTORE_CREDENTIALS", "./gen-lang-client-0000121060-ea7b2bef1534.json")
credentials = service_account.Credentials.from_service_account_file(cred_path)
DATABASE = firestore.Client(credentials=credentials)

class GeminiService:
    def __init__(self):
        try:
            if settings.GEMINI_API_KEY:
                genai.configure(api_key=settings.GEMINI_API_KEY)
                
                # System Instructions ì •ì˜
                system_instruction = """
                ë‹¹ì‹ ì€ ê²½ìƒë¶ë„ ì˜ì„±êµ° ì „ë¬¸ ai ê´€ê´‘ ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤.
                
                í•µì‹¬ ì„ë¬´:
                1. ì˜ì„±êµ° ê´€ê´‘ì •ë³´ ì „ë¬¸ê°€ë¡œì„œ ì •í™•í•˜ê³  ìœ ìš©í•œ ì •ë³´ ì œê³µ
                2. ì‚¬ìš©ì ì¿¼ë¦¬ë¥¼ ì •í™•íˆ ë¶„ì„í•˜ì—¬ ë§ì¶¤í˜• ê´€ê´‘ì§€ ì¶”ì²œ
                3. ì˜ì„±êµ°ì˜ ì§€ì—­ íŠ¹ìƒ‰ì„ ë°˜ì˜í•œ ê°œì¸í™”ëœ ì„œë¹„ìŠ¤ ì œê³µ
                
                ì‘ë‹µ ì›ì¹™:
                - í•­ìƒ JSON í˜•íƒœë¡œ êµ¬ì¡°í™”ëœ ì‘ë‹µ ì œê³µ
                - ì˜ì„±êµ°ì˜ ë¬¸í™”ì , ì§€ë¦¬ì  íŠ¹ì„±ì„ ê³ ë ¤
                - ì‹¤ìš©ì ì´ê³  ì‹¤í–‰ ê°€ëŠ¥í•œ ì •ë³´ë§Œ ì œê³µ
                - ì‚¬ìš©ìì˜ ì˜ë„ì™€ ë§¥ë½ì„ ì •í™•íˆ íŒŒì•…
                - 
                ** ì£¼ì˜ ì‚¬í•­ **:
                - ì‘ë‹µ ê°œìˆ˜: ë°˜ë“œì‹œ 15ê°œ ì´ìƒ
                - contenttypeid ê°’ì´ 39 ì¸ item (ìŒì‹ì ) : ë°˜ë“œì‹œ  3ê°œ ì´ìƒ í¬í•¨
                - contenttypeid ê°’ì´ 32 ì¸ item (ìˆ™ë°•ì‹œì„¤): ë°˜ë“œì‹œ ì „ë¶€ í¬í•¨
                
                ì˜ì„±êµ° ì „ë¬¸ ì§€ì‹:
                - ë§ˆëŠ˜, ì–‘íŒŒ íŠ¹ì‚°í’ˆê³¼ ê´€ë ¨ ê´€ê´‘ìì›
                - ì¡°ë¬¸êµ­ ì—­ì‚¬ë¬¸í™”ìœ ì  (ì¡°ë¬¸êµ­ì‚¬ì ì§€, ì¡°ë¬¸êµ­ë°•ë¬¼ê´€)
                - ìì—°ê´€ê´‘ì§€ (ë¹™ê³„ê³„ê³¡, ì‚¬ì´Œì—­ ì€í–‰ë‚˜ë¬´ê¸¸)
                - ì „í†µë¬¸í™”ì‹œì„¤ (ì˜ì„±í–¥êµ, ì˜ì„±ê´€ì•„)
                - ì²´í—˜ê´€ê´‘ í”„ë¡œê·¸ë¨ ë° ì‹œì„¤
                """
                
                # System Instructionsì™€ í•¨ê»˜ ëª¨ë¸ ì´ˆê¸°í™”
                self.model = genai.GenerativeModel(
                    'gemini-2.5-flash',
                    system_instruction=system_instruction
                )
                logger.info("Gemini AI initialized successfully with system instructions")
            else:
                logger.error("GEMINI_API_KEY not found in settings")
                self.model = None
        except Exception as e:
            logger.error(f"Failed to initialize Gemini AI: {e}")
            self.model = None
            
    def process_query(self, user_query):
        try:
            db = DATABASE
            collections = db.collection('tour_list').get()
            all_spots = [
                {
                    'id': collection.to_dict().get('contentid', collection.id),  # contentid ì‚¬ìš©, ì—†ìœ¼ë©´ ë¬¸ì„œ ID ì‚¬ìš©
                    'name': collection.to_dict().get('title', ''),  # titleì„ nameìœ¼ë¡œ ì‚¬ìš©
                    'overview': collection.to_dict().get('overview', ''),
                    'category': collection.to_dict().get('category', ''),
                } for collection in collections if collection.to_dict().get('contentid') or collection.id
            ]
            
            recommendation_result = self.recommend_tourism_spots(user_query, all_spots)

            full_spots = []
            for spot in recommendation_result.get('recommended_spots', []):
                try:
                    spot_id = spot.get('id', '')
                    if not spot_id:
                        logger.warning(f"Spot ID is empty, skipping: {spot}")
                        continue
                        
                    # contentidë¡œ ë¬¸ì„œ ì°¾ê¸° (ìƒˆë¡œìš´ filter ë°©ì‹ ì‚¬ìš©)
                    docs = db.collection('tour_list').where(filter=FieldFilter('contentid', '==', spot_id)).get()
                    
                    if docs:
                        # contentidë¡œ ì°¾ì€ ê²½ìš°
                        item = docs[0].to_dict()
                    else:
                        # ë¬¸ì„œ IDë¡œ ì§ì ‘ ì°¾ê¸°
                        doc = db.collection('tour_list').document(spot_id).get()
                        if doc.exists:
                            item = doc.to_dict()
                        else:
                            logger.warning(f"Document not found for ID: {spot_id}")
                            continue
                    
                    full_spots.append({
                        'addr1': item.get('addr1', ''),
                        'addr2': item.get('addr2', ''),
                        'areacode': item.get('areacode', ''),
                        'cat1': item.get('cat1', ''),
                        'cat2': item.get('cat2', ''),
                        'cat3': item.get('cat3', ''),
                        'contentid': item.get('contentid', ''),
                        'contenttypeid': item.get('contenttypeid', ''),
                        'createdtime': item.get('createdtime', ''),
                        'firstimage': item.get('firstimage', ''),
                        'firstimage2': item.get('firstimage2', ''),
                        'mapx': item.get('mapx', ''),
                        'mapy': item.get('mapy', ''),
                        'tel': item.get('tel', ''),
                        'title': item.get('title', ''),
                        'zipcode': item.get('zipcode', ''),
                        'overview': item.get('overview', ''),
                        'price': item.get('price', ''),
                    })
                except Exception as e:
                    logger.error(f"Error fetching full spot details for {spot.get('id', 'unknown')}: {e}")
                    continue
                
            recommendation_result['recommended_spots'] = full_spots
            
            response_data = {
                    'success': True,
                    'query': user_query,
                    'analysis': recommendation_result.get('analysis', {}),
                    'recommended_spots': recommendation_result.get('recommended_spots', []),
            }
            return response_data
        except Exception as e:
            logger.error(f"Error processing query: {e}")
            return {'success': False, 'message': str(e)}

    def recommend_tourism_spots(self, user_query: str, all_spots: list, max_results: int = 30) -> Dict:
        """ì‚¬ìš©ì ì¿¼ë¦¬ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ê´€ê´‘ì§€ ì¶”ì²œ"""
        try:
            # 1. ì¿¼ë¦¬ ë¶„ì„
            analysis_result = self.analyze_user_query(user_query)
            
            if not analysis_result['success']:
                return analysis_result
            
            analysis = analysis_result['analysis']
            
            # 2. ê°„ë‹¨í•œ í‚¤ì›Œë“œ ë§¤ì¹­ìœ¼ë¡œ ê´€ê´‘ì§€ ì¶”ì²œ
            recommended_spots = []
            keywords = analysis.get('keywords', [])
            
            for spot in all_spots:
                # í‚¤ì›Œë“œê°€ ì´ë¦„ì´ë‚˜ ê°œìš”ì— í¬í•¨ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸
                spot_text = f"{spot.get('name', '')} {spot.get('overview', '')}".lower()
                for keyword in keywords:
                    if keyword.lower() in spot_text:
                        recommended_spots.append(spot)
                        break
            
            # í‚¤ì›Œë“œ ë§¤ì¹­ì´ ì—†ìœ¼ë©´ ì „ì²´ ëª©ë¡ì—ì„œ ì¼ë¶€ë§Œ ë°˜í™˜
            if not recommended_spots:
                recommended_spots = all_spots[:max_results]
            
            # ê²°ê³¼ ìˆ˜ ì œí•œ
            recommended_spots = recommended_spots[:max_results]
            
            return {
                'success': True,
                'query': user_query,
                'analysis': analysis,
                'recommended_spots': recommended_spots,
                'total_found': len(recommended_spots),
                'returned_count': len(recommended_spots)
            }
            
        except Exception as e:
            logger.error(f"Error recommending tourism spots: {e}")
            return {'success': False, 'message': str(e)}
    
    def analyze_user_query(self, user_query: str) -> Dict:
        """ì‚¬ìš©ì ì¿¼ë¦¬ë¥¼ ë¶„ì„í•˜ì—¬ ê´€ê´‘ì§€ ê²€ìƒ‰ ì¡°ê±´ ì¶”ì¶œ"""
        try:
            # Gemini AI ìƒíƒœ ìƒì„¸ ë¡œê¹…
            logger.info(f"=== Gemini AI ìƒíƒœ í™•ì¸ ===")
            logger.info(f"self.model ì¡´ì¬: {self.model is not None}")
            logger.info(f"GEMINI_API_KEY ì„¤ì •: {bool(getattr(settings, 'GEMINI_API_KEY', None))}")
            
            # if not self.model:
            #     logger.warning("âš ï¸ Gemini AI not available, using fallback analysis")
            #     logger.warning("Gemini AIë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ì–´ í´ë°± ë¶„ì„ì„ ì‚¬ìš©í•©ë‹ˆë‹¤")
            #     return self._fallback_analysis(user_query)
            
            logger.info("âœ… Gemini AI ì‚¬ìš© ê°€ëŠ¥ - ì‹¤ì œ AI ë¶„ì„ ì‹œì‘")
            
            # ì˜ì„±êµ° ê´€ê´‘ì§€ ì •ë³´ë¥¼ ê¸°ë°˜ìœ¼ë¡œ í•œ í”„ë¡¬í”„íŠ¸ ìƒì„±
            prompt = self._create_analysis_prompt(user_query)
            logger.info(f"Generated prompt length: {len(prompt)}")
            
            # Generation config ì„¤ì • (ë” ì •í™•í•˜ê³  ì¼ê´€ëœ ì‘ë‹µì„ ìœ„í•´)
            generation_config = genai.types.GenerationConfig(
                temperature=0.8,  #ì¼ê´€ì„± ìˆëŠ” ì‘ë‹µ ì •ë„ (ë†’ì„ìˆ˜ë¡ ììœ ë„)
                top_p=0.8,
                top_k=40,
                max_output_tokens=10000,  # í† í° ìˆ˜ ì¤„ì„
                response_mime_type="text/plain"
            )
            
            # ì•ˆì „ì„± ì„¤ì • (í•„í„°ë§ ì™„í™”)
            safety_settings = [
                {
                    "category": "HARM_CATEGORY_HARASSMENT",
                    "threshold": "BLOCK_ONLY_HIGH"
                },
                {
                    "category": "HARM_CATEGORY_HATE_SPEECH",
                    "threshold": "BLOCK_ONLY_HIGH"
                },
                {
                    "category": "HARM_CATEGORY_SEXUALLY_EXPLICIT",
                    "threshold": "BLOCK_ONLY_HIGH"
                },
                {
                    "category": "HARM_CATEGORY_DANGEROUS_CONTENT",
                    "threshold": "BLOCK_ONLY_HIGH"
                }
            ]
            
            logger.info("ğŸ¤– Gemini AI í˜¸ì¶œ ì¤‘...")
            response = self.model.generate_content(
                prompt,
                generation_config=generation_config,
                safety_settings=safety_settings
            )
            
            # ì‘ë‹µ ìƒíƒœ í™•ì¸
            if not response.candidates:
                logger.error("âŒ Gemini AI ì‘ë‹µì— í›„ë³´ê°€ ì—†ìŠµë‹ˆë‹¤")
                raise Exception("Gemini AI returned no candidates")
            
            candidate = response.candidates[0]
            finish_reason = candidate.finish_reason
            
            # finish_reason í™•ì¸ (0: FINISH_REASON_UNSPECIFIED, 1: FINISH_REASON_STOP, 2: FINISH_REASON_MAX_TOKENS, 3: FINISH_REASON_SAFETY, 4: FINISH_REASON_RECITATION)
            if finish_reason == 1:  # FINISH_REASON_STOP - ì •ìƒ ì™„ë£Œ
                logger.info(f"âœ… Gemini AI ì‘ë‹µ ë°›ìŒ: {response.text[:100]}...")
                response_text = response.text
            elif finish_reason == 2:  # FINISH_REASON_MAX_TOKENS
                logger.warning("âš ï¸ Gemini AI ì‘ë‹µì´ ìµœëŒ€ í† í° ìˆ˜ì— ë„ë‹¬í–ˆìŠµë‹ˆë‹¤")
                if candidate.content and candidate.content.parts:
                    response_text = candidate.content.parts[0].text
                else:
                    raise Exception("Response truncated due to max tokens but no content available")
            elif finish_reason == 3:  # FINISH_REASON_SAFETY
                logger.error("âŒ Gemini AI ì‘ë‹µì´ ì•ˆì „ì„± í•„í„°ì— ì˜í•´ ì°¨ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤")
                raise Exception("Content blocked by safety filters")
            elif finish_reason == 4:  # FINISH_REASON_RECITATION
                logger.error("âŒ Gemini AI ì‘ë‹µì´ ì €ì‘ê¶Œ ë¬¸ì œë¡œ ì°¨ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤")
                raise Exception("Content blocked due to recitation")
            else:
                logger.error(f"âŒ ì•Œ ìˆ˜ ì—†ëŠ” finish_reason: {finish_reason}")
                raise Exception(f"Unknown finish reason: {finish_reason}")
                
            # ì‘ë‹µ íŒŒì‹±
            analysis_result = self._parse_analysis_response(response_text)
            
            logger.info(f"Query analysis completed for: {user_query}")
            logger.info(f"Analysis result: {analysis_result}")
            
            return {
                'success': True,
                'original_query': user_query,
                'analysis': analysis_result,
                'processed_query': analysis_result.get('processed_query', user_query),
                'gemini_used': True  # Geminiê°€ ì‹¤ì œë¡œ ì‚¬ìš©ë˜ì—ˆìŒì„ í‘œì‹œ
            }
            
        except Exception as e:
            logger.error(f"âŒ Error analyzing user query with Gemini: {e}")
            import traceback
            logger.error(f"Gemini ì˜¤ë¥˜ ìƒì„¸: {traceback.format_exc()}")
            return {
                'success': False,
                'message': str(e),
                'original_query': user_query,
                'analysis': {
                    'keywords': [],
                    'categories': [],
                    'preferences': [],
                    'intent': 'general_search',
                    'processed_query': user_query,
                    'confidence': 0.5
                },
                'gemini_used': False  # Geminiê°€ ì‚¬ìš©ë˜ì§€ ì•Šì•˜ìŒì„ í‘œì‹œ
            }
        
        
    def _create_analysis_prompt(self, user_query: str) -> str:
        """ì‚¬ìš©ì ì¿¼ë¦¬ ë¶„ì„ì„ ìœ„í•œ í”„ë¡¬í”„íŠ¸ ìƒì„±"""
        # ê°„ë‹¨í•˜ê³  ì•ˆì „í•œ í”„ë¡¬í”„íŠ¸
        prompt = f"""
ì‚¬ìš©ì ê´€ê´‘ ì¿¼ë¦¬ ë¶„ì„:

ì…ë ¥: "{user_query}"

ë‹¤ìŒ JSON í˜•ì‹ìœ¼ë¡œ ë¶„ì„ ê²°ê³¼ë¥¼ ì œê³µí•´ì£¼ì„¸ìš”:

{{
    "keywords": ["ê´€ë ¨ í‚¤ì›Œë“œë“¤"],
    "categories": ["ê´€ê´‘ì§€ ìœ í˜•"],
    "intent": "ì‚¬ìš©ì ì˜ë„",
    "processed_query": "ì •ì œëœ ì¿¼ë¦¬",
    "confidence": 0.8
}}

ì˜ì„±êµ° ê´€ê´‘ ì¹´í…Œê³ ë¦¬:
- ë¬¸í™”ì¬/ìœ ì ì§€
- ìì—°ê´€ê´‘ì§€  
- ì²´í—˜ê´€ê´‘ì§€
- ìŒì‹/ë§›ì§‘
- ìˆ™ë°•ì‹œì„¤
- ë ˆì €/ìŠ¤í¬ì¸ 

ë°˜ë“œì‹œ 15ê°œ ì´ìƒì˜ ì‘ë‹µì„ ì£¼ì„¸ìš”.
        """
        
        return prompt
    
    def _parse_analysis_response(self, response_text: str) -> Dict:
        """AI ì‘ë‹µì„ íŒŒì‹±í•˜ì—¬ êµ¬ì¡°í™”ëœ ë°ì´í„°ë¡œ ë³€í™˜"""
        try:
            # JSON ë¶€ë¶„ ì¶”ì¶œ (markdown ì½”ë“œ ë¸”ë¡ ì²˜ë¦¬ í¬í•¨)
            if '```json' in response_text:
                start_idx = response_text.find('```json') + 7
                end_idx = response_text.find('```', start_idx)
                json_str = response_text[start_idx:end_idx].strip()
            elif '```' in response_text:
                start_idx = response_text.find('```') + 3
                end_idx = response_text.find('```', start_idx)
                json_str = response_text[start_idx:end_idx].strip()
            else:
                start_idx = response_text.find('{')
                end_idx = response_text.rfind('}') + 1
                json_str = response_text[start_idx:end_idx] if start_idx != -1 and end_idx != -1 else response_text
            
            if json_str and json_str.startswith('{'):
                parsed_data = json.loads(json_str)
                
                # ê¸°ë³¸ í•„ë“œ ê²€ì¦ ë° ê¸°ë³¸ê°’ ì„¤ì •
                result = {
                    'keywords': parsed_data.get('keywords', []),
                    'categories': parsed_data.get('categories', []),
                    'preferences': parsed_data.get('preferences', []),
                    'intent': parsed_data.get('intent', 'general_search'),
                    'processed_query': parsed_data.get('processed_query', ''),
                    'confidence': float(parsed_data.get('confidence', 0.7))
                }
                
                # ë°ì´í„° í’ˆì§ˆ ê²€ì¦
                if not result['keywords']:
                    result['keywords'] = ['ê´€ê´‘', 'ì˜ì„±']
                if not result['processed_query']:
                    result['processed_query'] = response_text.strip()
                    
                return result
            else:
                raise json.JSONDecodeError("No valid JSON found", response_text, 0)
                
        except (json.JSONDecodeError, ValueError) as e:
            logger.warning(f"Failed to parse AI response as JSON: {e}")
            logger.warning(f"Response text: {response_text[:200]}...")
            
            # í´ë°± ë¶„ì„ - í…ìŠ¤íŠ¸ì—ì„œ í‚¤ì›Œë“œ ì¶”ì¶œ ì‹œë„
            keywords = []
            text_lower = response_text.lower()
            
            # ì˜ì„±êµ° ê´€ë ¨ í‚¤ì›Œë“œ ì¶”ì¶œ
            uiseong_keywords = ['ë§ˆëŠ˜', 'ì–‘íŒŒ', 'ì¡°ë¬¸êµ­', 'ë¹™ê³„ê³„ê³¡', 'ì‚¬ì´Œì—­', 'ì€í–‰ë‚˜ë¬´', 'í–¥êµ', 'ê´€ê´‘', 'ë§›ì§‘', 'ìˆ™ë°•']
            for keyword in uiseong_keywords:
                if keyword in text_lower:
                    keywords.append(keyword)
            
            return {
                'keywords': keywords if keywords else ['ê´€ê´‘', 'ì˜ì„±'],
                'categories': [],
                'preferences': [],
                'intent': 'general_search',
                'processed_query': response_text.strip()[:100],
                'confidence': 0.4
            }



        